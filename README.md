# Financial-Fraud-Detection
### My capstone project for Fullstack Academy's Data Analysis program. Analyzes ~100MB credit card transaction data for fraud patterns using Jupyter Notebook (Python preprocessing, MySQL batch insertion), SQL scripts, Tableau dashboard, and Excel files.

This repository is my capstone project for my Data Analysis program at Fullstack Academy. The dataset contained 389,002 credit card transactions across 23 features,, including timestamps, amounts, categories, demographics, and fraud labels. My analytical process included data importation, significant and rigorous data preprocessing, querying in MySQL, and dynamic visualizatins in Tableau Public. 

Below, I will detail the process that I took and the key insights I derived during this process.

#### Initial Attempt and the Large Dataset Challenge
I started this process by opening the cc_fraud_data.csv file in Excel as there were tasks that I needed to accomplish in Excel first for my Capstone project. As soon as I started with the dataset, I realized that there were issues with the timestamped data (specifically the trans_date_trans_time column and the dob column. The issue was that the these columns were in mixed formats (e.g., 09-06-2025 01:05 as DD-MM-YYYY HH:MM, 06/09/2025 01:05 as MM/DD/YYYY HH:MM). The issue was that I could not analyze the data becuase of the mixed formatting. I attempted to fix this issue in Excel, however I was unable to do this properly chiefly because the program was freezing and could not process my requests. This also happened with simple filtering, which I quickly assumed was due to the large size of the file. The sorting of the time-date columns was also difficult because I could not clearly identify or distinguish a pattern with the timestamps. Some indiviauls in my cohort at Fullstack were having similar issues, and our instructors offered a smaller 60,000 row dataset, however, one of the biggest reasons I selected this Capstone project was due to the size of the dataset. I decided that I would find a solution to my problem and would teach me something. I will note, that I had absolutely no idea what I was getting myself into but I was committed to figuring it out. 

#### Failed MySQL Import: Workbench Limitations
I figured, if Excel would not work - I could use MySQL Workbench, which I needed to use anyway for the Capstone, to process and clean the data. In our program we learned that we could import datasets using commands and the Table Import Wizard, and that is exactly what I proceeded to do. I created a transactions table in a 'finance' database. The import was slow, extremely slow, and I woke up the next morning and found that the Workbench has crashed. No luck. This setback echoed the limitations with Excel, and I recalled early lectures on Python's ability to handle large datasets, so I shifted my approach to a Python and started looking at ways to clean and import the data efficiently. 

#### Data Import and Preprocessing in Python 
Loading the data into Python was a breeze - no difficulty there. I immediately was able to see the dataset, view the features, and was able to confirm that there were no missing values in the dataset. So my attention immediately shifted to the timedate columns. I inspected the trans_date_trans_time column, and found these two mixed formats: DD-MM-YYYY HH:MM and MM/DD/YYYY HH:MM. My first inclination was to use the 'pd.to_datetime' command which produced a solution that worked for half the rows, the other half produced NaT errors. So I referred to the documentation - and realized that I was going to need a function to parse both formats. 

    
